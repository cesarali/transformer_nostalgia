{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd4b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    DownloadMode,\n",
    "    get_dataset_split_names,\n",
    "    load_dataset,\n",
    "    load_dataset_builder,\n",
    ")\n",
    "\n",
    "from reasoningschema import data_path\n",
    "raw_path = os.path.join(data_path,\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b0cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/cesar/Desktop/Projects/Reasoning/Code/ReasoningSchema/data/raw/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\",cache_dir=raw_path,split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe585ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57328a33b3a91d1900202e2b',\n",
       " 'title': 'Antibiotics',\n",
       " 'context': 'Antibiotics are screened for any negative effects on humans or other mammals before approval for clinical use, and are usually considered safe and most are well tolerated. However, some antibiotics have been associated with a range of adverse side effects. Side-effects range from mild to very serious depending on the antibiotics used, the microbial organisms targeted, and the individual patient. Side effects may reflect the pharmacological or toxicological properties of the antibiotic or may involve hypersensitivity reactions or anaphylaxis. Safety profiles of newer drugs are often not as well established as for those that have a long history of use. Adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis. Common side-effects include diarrhea, resulting from disruption of the species composition in the intestinal flora, resulting, for example, in overgrowth of pathogenic bacteria, such as Clostridium difficile. Antibacterials can also affect the vaginal flora, and may lead to overgrowth of yeast species of the genus Candida in the vulvo-vaginal area. Additional side-effects can result from interaction with other drugs, such as elevated risk of tendon damage from administration of a quinolone antibiotic with a systemic corticosteroid. Some scientists have hypothesized that the indiscriminate use of antibiotics alter the host microbiota and this has been associated with chronic disease.',\n",
       " 'question': 'Name some side-effects?',\n",
       " 'answers': {'text': ['diarrhea'], 'answer_start': [803]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7072385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'test']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_split_names(\"commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dbd3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset commonsense_qa (C:/Users/cesar/Desktop/Projects/Reasoning/Code/ReasoningSchema/data/raw/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"commonsense_qa\",\n",
    "                       cache_dir=raw_path,\n",
    "                       split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d38281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9741"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1468deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoningschema.data.dataloaders.reasoning import (\n",
    "    CommonsenseQADataLoader,\n",
    "    load_tokenizer,\n",
    ")\n",
    "from reasoningschema.utils.helper import load_prompting_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e468f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4720df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = load_tokenizer(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4371f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset commonsense_qa (C:/Users/cesar/Desktop/Projects/Reasoning/Code/ReasoningSchema/data/raw/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50817cbb8694caf8a8fb584778389db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\cesar\\Desktop\\Projects\\Reasoning\\Code\\ReasoningSchema\\data\\raw\\commonsense_qa\\default\\1.0.0\\28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b\\cache-a245e0542e0fce2d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\cesar\\Desktop\\Projects\\Reasoning\\Code\\ReasoningSchema\\data\\raw\\commonsense_qa\\default\\1.0.0\\28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b\\cache-38eece5cb48ef3c7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\cesar\\Desktop\\Projects\\Reasoning\\Code\\ReasoningSchema\\data\\raw\\commonsense_qa\\default\\1.0.0\\28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b\\cache-697de59697c8f085.arrow\n",
      "Loading cached processed dataset at C:\\Users\\cesar\\Desktop\\Projects\\Reasoning\\Code\\ReasoningSchema\\data\\raw\\commonsense_qa\\default\\1.0.0\\28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b\\cache-c057fc84e93f0e14.arrow\n",
      "Loading cached processed dataset at C:\\Users\\cesar\\Desktop\\Projects\\Reasoning\\Code\\ReasoningSchema\\data\\raw\\commonsense_qa\\default\\1.0.0\\28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b\\cache-b2c7e2e4a5977267.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = CommonsenseQADataLoader(batch_size=32,\n",
    "                                     test_batch_size=8, \n",
    "                                     supervised=True,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     root_dir=raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af150e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = getattr(dataloader, \"train\" + \"_it\")\n",
    "databatch = next(dataset.__iter__())\n",
    "len(databatch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99100cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '075e483d21c29a511267ef62bedc0461',\n",
       " 'question': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?',\n",
       " 'question_concept': 'punishing',\n",
       " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
       "  'text': ['ignore', 'enforce', 'authoritarian', 'yell at', 'avoid']},\n",
       " 'answerKey': 'A',\n",
       " 'text_q': 'Q: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? Answer Choices: (a) ignore\\n(b) enforce\\n(c) authoritarian\\n(d) yell at\\n(e) avoid',\n",
       " 'text_a': '\\nA: The answer is (a) ignore',\n",
       " 'input_ids': tensor([   48,    25,   383,  ..., 50257, 50257, 50257]),\n",
       " 'token_type_ids': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
