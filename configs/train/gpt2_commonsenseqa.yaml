experiment:
  name: GPT2-CSQA-LoRA
  seed: [1]
  device_map: cuda # auto, cuda, cpu

distributed:
  enabled: false

define: &backbone gpt2

model:
  name: LLMCausal
  backbone: *backbone
  backbone_path: null # if null it uses the default cache path set for huggingface
  load_in_8bit: false
  load_in_4bit: false
  use_bf16: false
  peft:
    method: lora # null, lora, ...
    r: 32
    lora_alpha: 32
    # target_modules: !!python/tuple ["c_attn", "c_proj", "c_fc"]
    bias: none
    task_type: CAUSAL_LM
    lora_dropout: 0.05
    inference_mode: false

tokenizer:
  name: *backbone
  cache_dir: null
  padding_side: right # left or right
  add_pad_token: true

dataset:
  name: commonsense_qa
  root_dir: null # if null it uses the default cache path set for huggingface
  batch_size: 128
  test_batch_size: 256
  num_workers: 2
  supervised: true # if false it appends "\nA: The answer is " else "\nA: The answer is <(a) correct answer>"
  prompt_path: null # prompts/commonsenseQA_SP.txt # path to a text file where the prompts are stored
  max_padding_length: 130
  target_type: SEQ2SEQ
  output_fields: !!python/tuple ["input_ids", "attention_mask", "labels"]
  force_download: false

optimizers: !!python/tuple
  - optimizer: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.00005
      weight_decay: 0.0
      gradient_norm_clipping: 0.1
      schedulers: !!python/tuple
        - name: torch.optim.lr_scheduler.StepLR
          step_size: 50
          gamma: 0.8
          step_type: minibatch

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: 40
  save_every: 1
  gradient_accumulation_steps: 2
  detect_anomaly: false
  best_metric: ppl
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  schedulers: !!python/tuple
    - name: reasoningschema.utils.param_scheduler.PeriodicScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: beta_scheduler_kl_rws
    - name: reasoningschema.utils.param_scheduler.ExponentialSchedulerGumbel
      label: temperature_scheduler_rws
      init_temperature: 1
      min_temperature: 0.5
      training_fraction_to_reach_min: 0.7
