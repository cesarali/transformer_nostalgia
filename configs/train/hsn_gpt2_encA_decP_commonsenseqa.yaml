experiment:
  name: HSN-GPT2-ENCA-DECP-CSQA
  seed: [1]
  device_map: null # auto, cuda, cpu

distributed:
  enabled: true
  sharding_strategy: FULL_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: MODEL_SPECIFIC # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

define: &backbone gpt2
model:
  name: HSN
  n_symbols: 50
  word_dropout: 0.3 # 0.0
  onehot_symbols: false
  rw_length: 10 # MUST BE MULTIPLE OF 2
  common_backbone:
    name: reasoningschema.models.blocks.HFBlock
    backbone: null # null if you do not want to use a common background for the encoder and the decoder
    freeze_backbone: false
  encoder:
    name: reasoningschema.models.blocks.encoders.EncoderModelA
    backbone: *backbone
    constrain_posterior_with_kl: true
    use_data_dependent_prior: true
    n_layers_prior: 2
    global_reasoning_prob: true
    peft:
      method: null # null, lora, ...
      r: 32
      lora_alpha: 32
      bias: none
      task_type: CAUSAL_LM
      lora_dropout: 0.05
      inference_mode: false
  decoder:
    name: reasoningschema.models.blocks.decoders.PseudoAttentionDecoder
    backbone: *backbone
    peft:
      method: null # null, lora, ...
      r: 32
      lora_alpha: 32
      bias: none
      task_type: CAUSAL_LM
      lora_dropout: 0.05
      inference_mode: false
  graph:
    name: reasoningschema.models.blocks.GraphGenerator
    symbols2hidden_layers: !!python/tuple [512, 512] # must be list!
    prior_edge_prob: 0.2
    sparse_prior_model: false
    hard_samples: true
    nonlinearity: LeakyReLU
    normalization: false
    diag_in_adj_matrix: false
    constrain_posterior_with_kl: true
    aggregated_kl: true # if kl_graph is computed using the average posterior link prob

tokenizer:
  name: gpt2
  # name: meta-llama/Llama-2-7b-hf
  cache_dir: null
  padding_side: right # left or right
  add_pad_token: true

dataset:
  name: commonsense_qa
  root_dir: null # if null it uses the default cache path set for huggingface
  batch_size: 128
  test_batch_size: 256
  num_workers: 2
  supervised: true # if false it appends "\nA: The answer is " else "\nA: The answer is <(a) correct answer>"
  prompt_path: null # prompts/commonsenseQA_SP.txt # path to a text file where the prompts are stored
  max_padding_length: 120
  target_type: SEQ2SEQ
  output_fields:
    !!python/tuple ["input_ids", "attention_mask", "token_type_ids", "labels"]
  force_download: false

optimizers: !!python/tuple
  - optimizer: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.00005
      weight_decay: 0.0
      gradient_norm_clipping: 0.1
      schedulers: !!python/tuple
        - name: torch.optim.lr_scheduler.StepLR
          step_size: 10
          gamma: 0.8
          step_type: epoch

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16 #fp32_policy # fp32_policy # bf16
  epochs: 40
  gradient_accumulation_steps: 2
  detect_anomaly: false
  save_every: 2
  best_metric: ppl
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  schedulers: !!python/tuple
    - name: reasoningschema.utils.param_scheduler.PeriodicScheduler
      label: weight_kl_rws
      max_value: 1.0
      max_steps_type: epoch # epoch or full
    - name: reasoningschema.utils.param_scheduler.ExponentialSchedulerGumbel
      label: temp_rws
      init_temperature: 0.75
      min_temperature: 0.75
      training_fraction_to_reach_min: 0.7
    ################# GRAPH ##########################
    - name: reasoningschema.utils.param_scheduler.PeriodicScheduler
      label: weight_kl_graph
      max_value: 1.0 # for exp./periodic sched.
      max_steps_type: epoch # epoch or full
    - name: reasoningschema.utils.param_scheduler.ExponentialSchedulerGumbel
      label: temp_graph
      init_temperature: 0.75
      min_temperature: 0.75
      training_fraction_to_reach_min: 0.7
